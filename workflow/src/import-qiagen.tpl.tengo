ll := import("@platforma-sdk/workflow-tengo:ll")
self := import("@platforma-sdk/workflow-tengo:tpl.light")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
smart := import("@platforma-sdk/workflow-tengo:smart")
slices := import("@platforma-sdk/workflow-tengo:slices")
maps := import("@platforma-sdk/workflow-tengo:maps")
assets := import("@platforma-sdk/workflow-tengo:assets")
exec := import("@platforma-sdk/workflow-tengo:exec")
pt := import("@platforma-sdk/workflow-tengo:pt")
sets := import("@platforma-sdk/workflow-tengo:sets")
json := import("json")

self.defineOutputs("tsv", "stats")

self.body(func(inputs) {
	inputFiles := inputs[pConstants.VALUE_FIELD_NAME]
	columnsInfo := inputs.columnsInfo
	chains := inputs.chains
	config := inputs.config
	xsvType := "tsv"
	if !is_undefined(config) && !is_undefined(config.xsvType) { xsvType = config.xsvType }

	wf := pt.workflow().mem("8GiB").cpu(4)

	dataFrames := []
	for key, file in inputFiles.inputs() {
		dataFrames = append(dataFrames, wf.frame(file, {
			xsvType: xsvType,
			id: "table_" + key,
			schema: [{column: "read set", type: "String"}]
		}))
	}

	if len(dataFrames) == 0 {
		ll.panic("no input files found")
	}

	df := pt.concat(dataFrames)

	// Rename original columns to their target IDs for downstream processing.
	renameExpressions := []
	for canonical, original in columnsInfo.columnMapping {
		renameExpressions = append(renameExpressions, pt.col(original).alias(canonical))
	}
	df = df.withColumns(renameExpressions...)

	// For Qiagen data, remove the last 'C' from chain column values to make them compatible
	// TRAC -> TRA, TRBC -> TRB, TRGC -> TRG, TRDC -> TRD, IGHC -> IGH, IGKC -> IGK, IGLC -> IGL
	df = df.withColumns(
		pt.col("chain").strSlice(0, pt.col("chain").strLenChars().minus(1)).alias("chain")
	)

	// Drop rows with empty j-gene or v-gene
	for col in ["j-gene", "v-gene"] {
		df = df.filter(
			pt.col(col).isNotNull().and(pt.col(col).neq(""))
		)
	}
	
	// For Qiagen data, use the chain column to filter chains (after removing 'C' suffix)
	chainToLocusMap := {
		"IGHeavy": "IGH",
		"IGLight": "IGL", // Special case handled below
		"TCRAlpha": "TRA",
		"TCRBeta": "TRB", 
		"TCRGamma": "TRG",
		"TCRDelta": "TRD"
	}

	keyComponents := slices.map(columnsInfo.clonotypeKeyColumns, func(id) { return pt.col(id).fillNull("NA") })
	
	clonotypeKeyExpr := pt.
						concatStr(keyComponents, {delimiter: "|"}).
						hash("sha256", "base64_alphanumeric", 120).
						alias("clonotypeKey")

	for chain in chains {
		prefix := chainToLocusMap[chain]
		
		chainDf := undefined
		if chain == "IGLight" {
			// For IG light chains, check for both IGK and IGL (after removing 'C' suffix)
			chainDf = df.filter(
				pt.col("chain").eq("IGK").or(pt.col("chain").eq("IGL"))
			)
		} else {
			// Use exact chain column value for filtering (after removing 'C' suffix)
			qiagenChainValue := chainToLocusMap[chain]
			chainDf = df.filter(pt.col("chain").eq(qiagenChainValue))
		}

		// Calculate "is-productive" column based on CDR3 AA sequence presence.
		chainDf = chainDf.withColumns(
			pt.col("cdr3-aa").isNotNull().
				and(pt.col("cdr3-aa").neq("")).
				and(pt.col("cdr3-aa").strContains("*", {literal: true}).not()).
				and(pt.col("cdr3-aa").strContains("_", {literal: true}).not()).
				alias("is-productive")
		)

		// Filter for productive sequences.
		chainDf = chainDf.filter(pt.col("is-productive"))

		// Standardize productivity flag to lowercase string values
		chainDf = chainDf.withColumns(
			pt.when(pt.col("is-productive")).then(pt.lit("true")).otherwise(pt.lit("false")).alias("is-productive")
		)

		// For Qiagen, CDR3 nucleotide and amino acid sequences are already provided
		// Add length columns
		chainDf = chainDf.withColumns(
			pt.col("cdr3-aa").strLenChars().alias("cdr3-aa-length")
		)
		chainDf = chainDf.withColumns(
			pt.col("cdr3-nt").strLenChars().alias("cdr3-nt-length")
		)
		
		chainDf = chainDf.withColumns(clonotypeKeyExpr)

		// Ensure correct data type for UMI count
		chainDf = chainDf.withColumns(pt.col("umi-count").cast("Int64").alias("umi-count"))

		// Aggregate by clonotype key
		aggregations := []
		aggregations = append(aggregations, pt.col("umi-count").sum().alias("umi-count"))
		
		for col in columnsInfo.propertyColumns {
			aggregations = append(aggregations, pt.col(col.column).maxBy("umi-count").alias(col.column))
		}
		chainDf = chainDf.groupBy("clonotypeKey").agg(aggregations...)

		// Calculate UMI fraction
		chainDf = chainDf.withColumns(pt.col("umi-count").truediv(pt.col("umi-count").sum()).fillNaN(0).alias("umi-fraction"))

		// Keep only property and abundance columns
		keepColumns := ["clonotypeKey"]
		for col in columnsInfo.propertyColumns {
			keepColumns = append(keepColumns, col.column)
		}
		for col in columnsInfo.abundanceColumns {
			keepColumns = append(keepColumns, col.column)
		}
		chainDf = chainDf.select(keepColumns...)
		
		chainDf.save(chain + ".tsv")

		statsExprs := [
			pt.col("clonotypeKey").count().alias("clonotype-count"),
			pt.col("umi-count").sum().alias("umi-count")
		]
		stats := chainDf.select(statsExprs...)

		stats.saveContent("stats-" + chain + ".tsv")
	}

	wf = wf.run()

	tsv := {}
	stats := {}
	for chain in chains {
		tsv[chain] = wf.getFile(chain + ".tsv")
		stats[chain] = wf.getFileContent("stats-" + chain + ".tsv")
	}

    return {
		tsv: tsv,
		stats: stats
	}
})
