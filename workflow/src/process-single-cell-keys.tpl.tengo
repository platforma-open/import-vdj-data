ll := import("@platforma-sdk/workflow-tengo:ll")
self := import("@platforma-sdk/workflow-tengo:tpl.light")
pConstants := import("@platforma-sdk/workflow-tengo:pframes.constants")
assets := import("@platforma-sdk/workflow-tengo:assets")
maps := import("@platforma-sdk/workflow-tengo:maps")
slices := import("@platforma-sdk/workflow-tengo:slices")
units := import("@platforma-sdk/workflow-tengo:units")
pt := import("@platforma-sdk/workflow-tengo:pt")

clonotypeLabel := import(":clonotype-label")

json := import("json")
math := import("math")

// Outputs mirror @mixcr-clonotyping single-cell stage so that upstream UI can reuse surfaces
// For SC, this template returns files that upstream step will convert to Xsv with proper axes.
self.defineOutputs("abundanceTsv", "clonotypeTsv",
	"propertiesAPrimaryTsv", "propertiesASecondaryTsv", "propertiesBPrimaryTsv", "propertiesBSecondaryTsv",
	"cellsTsv")

self.body(func(inputs) {
	// Inputs:
	//  - value: byCellTagA (Map<[sampleId], File>) with columns: cellKey, clonotypeKey, mainAbundanceColumn, mainIsProductiveColumn
	//  - byCellTagB: same for the paired chain
	//  - propertiesA / propertiesB: aggregated per-chain properties (Map<[], File>) with clonotypeKey and schemaPerClonotypeNoAggregates
	//  - params: { mainAbundanceColumn, mainIsProductiveColumn, schemaPerClonotypeNoAggregates }

	byCellTagA := inputs[pConstants.VALUE_FIELD_NAME]
	inputDataMeta := byCellTagA.getDataAsJson()
	ll.assert(inputDataMeta.keyLength == 1, "unexpected number of aggregation axes")

	byCellTagB := inputs.byCellTagB
	propertiesA := inputs.propertiesA
	propertiesB := inputs.propertiesB

	mainAbundanceColumn := inputs.params.mainAbundanceColumn
	mainIsProductiveColumn := inputs.params.mainIsProductiveColumn
	schemaPerClonotypeNoAggregates := inputs.params.schemaPerClonotypeNoAggregates

	preprocessByCell := func(byCellTag) {
		inputMap := byCellTag.inputs()
		numberOfSamples := len(inputMap)

		wf := pt.workflow().
			inMediumQueue().
			mem(int(math.max(numberOfSamples, 32)) * units.GiB).
			cpu(int(math.max(numberOfSamples, 16)))

		sampleDataFrames := []
		maps.forEach(inputMap, func(sKey, inputFile) {
			key := json.decode(sKey)
			ll.assert(len(key) == 1, "preprocessByCell: byCellTag key should have one element, got %v", key)
			sampleId := key[0]

			sampleDf := wf.frame(inputFile, {
				xsvType: "tsv",
				schema: [
					{ column: "cellKey", type: "String" },
					{ column: "clonotypeKey", type: "String" },
					{ column: mainAbundanceColumn, type: "Long" },
					{ column: mainIsProductiveColumn, type: "String" }
				],
				inferSchema: false
			}).withColumns(
				pt.col("cellKey"), pt.col("clonotypeKey"), pt.col(mainAbundanceColumn), pt.col(mainIsProductiveColumn)
			)

			sampleDfWithId := sampleDf.addColumns(pt.lit(sampleId).alias("sampleId"))
			sampleDataFrames = append(sampleDataFrames, sampleDfWithId)
		})

		ll.assert(len(sampleDataFrames) > 0, "No input files to process in preprocessByCell")

		concatenatedDf := pt.concat(sampleDataFrames)

		dfWithRawRank := concatenatedDf.withColumns(
			pt.rank(pt.col(mainAbundanceColumn), {descending: true}).
				over([pt.col("sampleId"), pt.col("cellKey")]).
				alias("rawChainRank")
		)

		dfFilteredByRawRank := dfWithRawRank.filter(pt.col("rawChainRank").le(2))

		isProductiveNumericExpr := pt.when(pt.col(mainIsProductiveColumn).eq("True")).
			then(pt.lit(0)).
			otherwise(pt.lit(1))
		negativeAbundanceExpr := pt.col(mainAbundanceColumn).multiply(-1)

		dfWithChainRank := dfFilteredByRawRank.withColumns(
			pt.rank([
					isProductiveNumericExpr,
					negativeAbundanceExpr
				], {descending: false}).
				over([pt.col("sampleId"), pt.col("cellKey")]).
				alias("chainRank")
		).withoutColumns("rawChainRank")

		dfWithChainRank.save("output.tsv")
		runResult := wf.run()
		return runResult.getFile("output.tsv")
	}

	chainAOutput := preprocessByCell(byCellTagA)
	chainBOutput := preprocessByCell(byCellTagB)

	cellGroupingWf := pt.workflow()
	chainATableDf := cellGroupingWf.frame(chainAOutput, {xsvType: "tsv"})
	chainBTableDf := cellGroupingWf.frame(chainBOutput, {xsvType: "tsv"})

	rankedDfs := {}
	for chainData in [{prefix: "a", df: chainATableDf}, {prefix: "b", df: chainBTableDf}] {
		for rankVal in [1, 2] {
			dfKey := "chain_" + chainData.prefix + "_rank" + string(rankVal) + "_df"
			rankedDfs[dfKey] = chainData.df.filter(pt.col("chainRank").eq(rankVal))
		}
	}

	chainAMergedDf := rankedDfs["chain_a_rank1_df"].join(rankedDfs["chain_a_rank2_df"], {
		how: "full",
		on: ["sampleId", "cellKey"],
		coalesce: true,
		leftColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyA1" }],
		rightColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyA2" }]
	})

	chainBMergedDf := rankedDfs["chain_b_rank1_df"].join(rankedDfs["chain_b_rank2_df"], {
		how: "full",
		on: ["sampleId", "cellKey"],
		coalesce: true,
		leftColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyB1" }],
		rightColumns: [{ column: "clonotypeKey", rename: "clonotypeKeyB2" }]
	})

	allChainsMergedDf := chainAMergedDf.join(chainBMergedDf, {
		how: "full",
		on: ["sampleId", "cellKey"],
		coalesce: true
	})

	scClonotypeKeyExpr := pt.concatStr([
		pt.col("clonotypeKeyA1").fillNull("NA"),
		pt.col("clonotypeKeyA2").fillNull("NA"),
		pt.col("clonotypeKeyB1").fillNull("NA"),
		pt.col("clonotypeKeyB2").fillNull("NA")
	], {delimiter: "#"}).hash("sha256", "base64_alphanumeric", 120).alias("scClonotypeKey")

	allChainsMergedWithScKeyDf := allChainsMergedDf.withColumns(scClonotypeKeyExpr)

	filterCondition := pt.and(
		pt.col("clonotypeKeyA1").isNotNull(),
		pt.col("clonotypeKeyB1").isNotNull()
	)
	allChainsFilteredDf := allChainsMergedWithScKeyDf.filter(filterCondition)

	allChainsFilteredDf.
		withColumns(pt.lit(1).alias("1")).
		save("cells.tsv", {columns: ["sampleId", "cellKey", "scClonotypeKey", "1"]})

	clonotypeTableDf := allChainsFilteredDf.groupBy(
		"scClonotypeKey", "clonotypeKeyA1", "clonotypeKeyA2", "clonotypeKeyB1", "clonotypeKeyB2"
	).agg(
		pt.col("sampleId").nUnique().alias("sampleCount")
	)
	clonotypeTableDf = clonotypeLabel.addClonotypeLabelColumnsPt(clonotypeTableDf, "scClonotypeKey", "clonotypeLabel", pt)
	clonotypeTableDf.save("clonotype.tsv")

	cellCountsDf := allChainsFilteredDf.groupBy("sampleId", "scClonotypeKey").agg(
		pt.col("cellKey").count().alias("uniqueCellCount")
	)
	cellCountsWithFractionDf := cellCountsDf.withColumns(
		pt.col("uniqueCellCount").truediv(pt.col("uniqueCellCount").sum().over("sampleId")).alias("uniqueCellFraction")
	)
	cellCountsWithFractionDf.save("abundance.tsv")

	cellGroupingRunResult := cellGroupingWf.run()

	clonotypeTsv := cellGroupingRunResult.getFile("clonotype.tsv")
	abundanceTsv := cellGroupingRunResult.getFile("abundance.tsv")
	cellsTsv := cellGroupingRunResult.getFile("cells.tsv")

	// Join properties for four positions A1/A2/B1/B2
	propertiesAFile := propertiesA.inputs()["[]"]
	propertiesBFile := propertiesB.inputs()["[]"]

	outputProcessingWf := pt.workflow()

	mainClonotypesDf := outputProcessingWf.frame(clonotypeTsv, {xsvType: "tsv", schema: [
		{ column: "scClonotypeKey", type: "String" },
		{ column: "clonotypeKeyA1", type: "String" },
		{ column: "clonotypeKeyA2", type: "String" },
		{ column: "clonotypeKeyB1", type: "String" },
		{ column: "clonotypeKeyB2", type: "String" },
		{ column: "sampleCount", type: "Int" }
	], inferSchema: false })

	propsADf := outputProcessingWf.frame(propertiesAFile, {xsvType: "tsv", schema: [{ column: "clonotypeKey", type: "String" }], inferSchema: false })
	propsBDf := outputProcessingWf.frame(propertiesBFile, {xsvType: "tsv", schema: [{ column: "clonotypeKey", type: "String" }], inferSchema: false })

	clonotypeColumnNames := []
	for cc in schemaPerClonotypeNoAggregates { clonotypeColumnNames = append(clonotypeColumnNames, cc.column) }
	finalOutputColumns := ["scClonotypeKey", "clonotypeKey"] + clonotypeColumnNames

	chainMappings := [
		{ chainKeyCol: "clonotypeKeyA1", propsTable: "props_a", finalOutFile: "properties_a_primary.tsv" },
		{ chainKeyCol: "clonotypeKeyA2", propsTable: "props_a", finalOutFile: "properties_a_secondary.tsv" },
		{ chainKeyCol: "clonotypeKeyB1", propsTable: "props_b", finalOutFile: "properties_b_primary.tsv" },
		{ chainKeyCol: "clonotypeKeyB2", propsTable: "props_b", finalOutFile: "properties_b_secondary.tsv" }
	]

	propsMapDfs := { "props_a": propsADf, "props_b": propsBDf }
	for mapping in chainMappings {
		filteredClonotypesDf := mainClonotypesDf.filter(pt.col(mapping.chainKeyCol).isNotNull())
		filteredClonotypesWithKeyDf := filteredClonotypesDf.withColumns(pt.col(mapping.chainKeyCol).alias("clonotypeKey"))
		joinedDf := propsMapDfs[mapping.propsTable].join(filteredClonotypesWithKeyDf, { how: "inner", on: ["clonotypeKey"] })
		joinedDf.save(mapping.finalOutFile, { columns: finalOutputColumns, xsvType: "tsv" })
	}

	outputProcessingRunResult := outputProcessingWf.run()

	return {
		abundanceTsv: abundanceTsv,
		clonotypeTsv: clonotypeTsv,
		cellsTsv: cellsTsv,
		propertiesAPrimaryTsv: outputProcessingRunResult.getFile("properties_a_primary.tsv"),
		propertiesASecondaryTsv: outputProcessingRunResult.getFile("properties_a_secondary.tsv"),
		propertiesBPrimaryTsv: outputProcessingRunResult.getFile("properties_b_primary.tsv"),
		propertiesBSecondaryTsv: outputProcessingRunResult.getFile("properties_b_secondary.tsv")
	}
})
